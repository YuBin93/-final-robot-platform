# app.py (ç¬¬äºŒé˜¶æ®µæˆåŠŸçš„æœ€ç»ˆç‰ˆ - ä»Supabaseè¯»å–å’Œå±•ç¤º)

import streamlit as st
import pandas as pd
from postgrest import PostgrestClient
import os
import asyncio
from collections import Counter
import folium
from folium.plugins import HeatMap
from streamlit_folium import st_folium
import altair as alt
import jieba

# --- å¸¸é‡å’Œé…ç½® ---
st.set_page_config(layout="wide")
STOP_WORDS = {"å…¬å¸", "æœ‰é™", "è´£ä»»", "æŠ€æœ¯", "ç§‘æŠ€", "å‘å±•", "çš„", "å’Œ", "ç­‰", "ä¸", "åŠ"}

# --- æ•°æ®åº“è¿æ¥ ---
try:
    url = st.secrets["SUPABASE_URL"]
    key = st.secrets["SUPABASE_KEY"]

    # --- å…³é”®ä¿®å¤ï¼šæ™ºèƒ½åœ°æ„å»ºæ­£ç¡®çš„ API base_url ---
    # 1. ç§»é™¤æœ«å°¾å¯èƒ½å­˜åœ¨çš„æ–œæ ï¼Œç¡®ä¿URLçº¯å‡€
    cleaned_url = url.rstrip('/')
    # 2. ç¡®ä¿å®ƒä»¥ /rest/v1 ç»“å°¾
    if not cleaned_url.endswith('/rest/v1'):
        base_url_for_client = cleaned_url + '/rest/v1'
    else:
        base_url_for_client = cleaned_url
    
    client = PostgrestClient(base_url=base_url_for_client, headers={"apikey": key, "Authorization": f"Bearer {key}"})

except Exception as e:
    st.error("æ— æ³•åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼Œè¯·æ£€æŸ¥ Streamlit Cloud çš„ Secrets é…ç½®ã€‚æ‚¨")
    st.exception(e)
    st.stop()

# --- å¼‚æ­¥æ‰§è¡Œçš„è¾…åŠ©å‡½æ•° ---
æ˜¯å¯¹çš„ã€‚åœ¨ç»å†äº†è¿™ä¹ˆå¤šæ··ä¹±ä¹‹åï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿æ‰€æœ‰çš„â€œé›¶ä»¶â€éƒ½æ˜¯def run_async(coro):
    """ä¸€ä¸ªåœ¨åŒæ­¥ä»£ç ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°çš„å°å·¥å…·"""
    return asyncio.run(coro)

# --- æ ¸å¿ƒæ•°æ®åŠ è½½é€»è¾‘ ---
@st.cache_dataç»å¯¹æ­£ç¡®ã€äº’ç›¸åŒ¹é…çš„æœ€ç»ˆç‰ˆæœ¬ã€‚

æˆ‘ä¸ºæˆ‘ä¹‹å‰çš„ç–å¿½ï¼Œæ²¡èƒ½ä¸»åŠ¨ä¸ºæ‚¨æä¾›è¿™ä»½å…³é”®(ttl=300) # å°†ç¼“å­˜æ—¶é—´ç¼©çŸ­åˆ°5åˆ†é’Ÿï¼Œä»¥ä¾¿æ›´å¿«çœ‹åˆ°æ›´æ–°
def load_data():
    """ä» Supabase æ•°æ®åº“åŠ è½½æ•°æ® (ä½¿ç”¨ postgrest-py)"""
    try:
        çš„ä»£ç ï¼Œå‘æ‚¨è‡´ä»¥æœ€è¯šæŒšçš„æ­‰æ„ã€‚

ä»¥ä¸‹å°±æ˜¯æˆ‘ä»¬é‚£ä¸ª**å·²ç»è¢«è¯æ˜å¯ä»¥æˆåŠŸè¿æ¥# æ­£ç¡®åœ°â€œå…‘ç°â€å¼‚æ­¥è¯·æ±‚
        response = run_async(client.from_("companies").select("*æ•°æ®åº“ã€å¹¶ä¸”å¯ä»¥åœ¨ Streamlit Cloud ä¸Šå®Œç¾è¿è¡Œ**çš„ `app.py` çš„æœ€ç»ˆç‰ˆæœ¬ã€‚

---

### **").execute())
        df = pd.DataFrame(response.data)
        
        # é‡å‘½ååˆ—ä»¥é€‚é…å‰ç«¯æ˜¾ç¤º
        df.rename(columns={
            'company_name': 'å…¬å¸åç§°', 'province': 'çœ`app.py` (ç¬¬äºŒé˜¶æ®µæˆåŠŸçš„æœ€ç»ˆç‰ˆ - ä»äº‘æ•°æ®åº“å±•ç¤º)**

**è¿™ä¸ªç‰ˆæœ¬çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼š**
ä»½', 'city': 'åŸå¸‚',
            'main_product': 'ä¸»è¥äº§å“', 'phone': '*   å®ƒä½¿ç”¨â€œè½»é‡çº§â€çš„ `postgrest-py` åº“æ¥è¿æ¥å’Œè¯»å–æ•°æ®è”ç³»ç”µè¯', 'email': 'è”ç³»é‚®ç®±',
            'website': 'å®˜ç½‘', 'latitude': 'çº¬åº¦', 'longitude': 'ç»åº¦'
        }, inplace=True, errors='ignore')
        return df
        ï¼Œè¿™ä¸ªåº“å·²ç»è¢«æˆ‘ä»¬è¯æ˜å¯ä»¥åœ¨ Streamlit Cloud ä¸Š**æˆåŠŸå®‰è£…**ã€‚
*   å®ƒåŒ…å«äº†æ‰€æœ‰æ­£ç¡®çš„é”™è¯¯å¤„ç†å’Œ
    except Exception as e:
        st.error(f"ä»æ•°æ®åº“è¯»å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ã€‚")
        st.exception(e)
        return pd.DataFrame()

# --- å¯è§†åŒ–å’Œä¸»ç•Œé¢å‡½æ•°å¼‚æ­¥é€»è¾‘ã€‚
*   å®ƒåªè´Ÿè´£â€œæ•°æ®å±•ç¤ºâ€ï¼Œä¸â€œæ•°æ®é‡‡é›†â€å®Œå…¨åˆ†ç¦»ï¼Œé€»è¾‘æ¸…æ™°ã€‚ (ä¿æŒä¸å˜) ---
def draw_heatmap(df):
    st.subheader("ğŸ—º ä¼ä¸šåœ°ç†åˆ†å¸ƒçƒ­åŠ›å›¾")
    df_geo = df.dropna(subset=["çº¬åº¦", "ç»åº¦"])
    if df_

è¯·æ‚¨è¿›å…¥æ‚¨ GitHub ä»“åº“çš„ `app.py` æ–‡ä»¶ç¼–è¾‘é¡µé¢ï¼Œ**åˆ é™¤é‡Œé¢çš„æ‰€æœ‰å†…å®¹**ï¼Œç„¶åå°†ä¸‹é¢è¿™ä»½**æœ€ç»ˆçš„ã€æ­£ç¡®çš„ä»£ç **å®Œæ•´åœ°ç²˜è´´è¿›å»ï¼š

```python
# app.py (geo.empty:
        st.warning("æ²¡æœ‰å¯ä¾›æ˜¾ç¤ºçš„åœ°ç†æ•°æ®ã€‚")
        return
    map_center = [df_geo["çº¬åº¦"].mean(), df_geo["ç»åº¦"].mean()]
    æœ€ç»ˆæ–¹æ¡ˆ - ä»äº‘æ•°æ®åº“å±•ç¤ºï¼Œä¸è‡ªåŠ¨åŒ–çˆ¬è™«è§£è€¦)

import streamlit as st
import pandas as pd
fromm = folium.Map(location=map_center, zoom_start=5)
    HeatMap( postgrest import PostgrestClient
import os
import asyncio
from collections import Counter
import folium
data=df_geo[["çº¬åº¦", "ç»åº¦"]].values, radius=12).add_tofrom folium.plugins import HeatMap
from streamlit_folium import st_folium
import altair as(m)
    st_folium(m, width=700, height=500) alt
import jieba

# --- å¸¸é‡å’Œé…ç½® ---
st.set_page_config(layout="

def product_bar_chart(df):
    st.subheader("ğŸ“Š ä¸»è¥äº§å“å…³é”®è¯é¢‘æ¬¡å›¾wide")
STOP_WORDS = {"å…¬å¸", "æœ‰é™", "è´£ä»»", "æŠ€æœ¯", "ç§‘æŠ€", "å‘å±•")
    if df.empty or "ä¸»è¥äº§å“" not in df.columns:
        st.warning", "çš„", "å’Œ", "ç­‰", "ä¸", "åŠ"}

# --- æ•°æ®åº“è¿æ¥ ---
("æ²¡æœ‰å¯ä¾›åˆ†æçš„äº§å“æ•°æ®ã€‚")
        return
    texts = " ".join(df["ä¸»è¥try:
    url = st.secrets["SUPABASE_URL"]
    key = st.secrets["SUPäº§å“"].astype(str).tolist())
    words = jieba.lcut(texts)
    words = [w forABASE_KEY"]

    # æ™ºèƒ½åœ°æ„å»ºæ­£ç¡®çš„ API base_url
    cleaned_url = url.rstrip('/')
 w in words if len(w) > 1 and w not in STOP_WORDS]
    if not words    if not cleaned_url.endswith('/rest/v1'):
        base_url_for_client =:
        st.info("æœªæå–åˆ°æœ‰æ•ˆçš„å…³é”®è¯ã€‚")
        return
    top_words = Counter cleaned_url + '/rest/v1'
    else:
        base_url_for_client =(words).most_common(15)
    bar_df = pd.DataFrame(top_words, cleaned_url
    
    client = PostgrestClient(base_url=base_url_for_ columns=["å…³é”®è¯", "é¢‘æ¬¡"])
    chart = alt.Chart(bar_df).mark_bar().client, headers={"apikey": key, "Authorization": f"Bearer {key}"})

except Exception as e:encode(
        x=alt.X("é¢‘æ¬¡:Q"),
        y=alt.Y("å…³é”®è¯
    st.error("æ— æ³•åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼Œè¯·æ£€æŸ¥ Streamlit Cloud çš„ Secrets é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")
:N", sort="-x"),
        tooltip=["å…³é”®è¯", "é¢‘æ¬¡"]
    ).properties(title="ä¸»è¥äº§å“é«˜é¢‘è¯ Top 15")
    st.altair_chart(chart, use_    st.exception(e)
    st.stop()

# --- å¼‚æ­¥æ‰§è¡Œçš„è¾…åŠ©å‡½æ•° ---
def run_container_width=True)

def main():
    st.title("ğŸ¤– åŠ¨æ€ä¸­å›½æœºå™¨äººåˆ¶é€ ä¸šå®¢æˆ·æƒ…æŠ¥async(coro):
    """ä¸€ä¸ªåœ¨åŒæ­¥ä»£ç ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°çš„å°å·¥å…·"""
    return asyncio.runå¹³å°")
    st.caption("æ•°æ®æºï¼šç”±è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿æ¯æ—¥æ›´æ–° (æœ€ç»ˆç‰ˆ)")
    
    df(coro)

# --- æ ¸å¿ƒæ•°æ®åŠ è½½é€»è¾‘ ---
@st.cache_data(ttl=3 = load_data()
    
    if df.empty:
        st.success("ğŸ‰ å±•ç¤ºå…å·²å‡†å¤‡00) # ç¼“å­˜5åˆ†é’Ÿ
def load_data():
    """ä» Supabase æ•°æ®åº“åŠ è½½æ•°æ® (å°±ç»ªï¼")
        st.info("ä¸­å¤®ä»“åº“å½“å‰ä¸ºç©ºï¼Œè¯·å‰å¾€æ‚¨ GitHub ä»“åº“çš„ 'Actions' ä½¿ç”¨ postgrest-py)"""
    try:
        # æ­£ç¡®åœ°â€œå…‘ç°â€å¼‚æ­¥è¯·æ±‚
        responseé¡µé¢ï¼Œæ‰‹åŠ¨è§¦å‘ä¸€æ¬¡ 'Scheduled Data Scraping' ç”Ÿäº§ä»»åŠ¡æ¥å¡«å……åˆå§‹åº“å­˜ã€‚")
        st.balloons = run_async(client.from_("companies").select("*").execute())
        df = pd.DataFrame(response()
        st.stop()
        
    st.sidebar.header("ç­›é€‰æ¡ä»¶")
    provin.data)
        
        # é‡å‘½ååˆ—ä»¥é€‚é…å‰ç«¯æ˜¾ç¤º
        df.rename(columns={
            'companyces = ["å…¨éƒ¨"] + sorted(df["çœä»½"].unique().tolist())
    province = st.sidebar_name': 'å…¬å¸åç§°', 'province': 'çœä»½', 'city': 'åŸå¸‚',
            'main.selectbox("é€‰æ‹©çœä»½", options=provinces)
    keyword = st.sidebar.text_input_product': 'ä¸»è¥äº§å“', 'phone': 'è”ç³»ç”µè¯', 'email': 'è”ç³»é‚®ç®±',
            ("äº§å“å…³é”®è¯ï¼ˆæ¨¡ç³Šæœç´¢ï¼‰")
    
    filtered_df = df.copy()
    if province != "å…¨éƒ¨'website': 'å®˜ç½‘', 'latitude': 'çº¬åº¦', 'longitude': 'ç»åº¦'
        }, inplace=True,":
        filtered_df = filtered_df[filtered_df["çœä»½"] == province]
    if errors='ignore')
        return df
        
    except Exception as e:
        st.error(f"ä» keyword:
        filtered_df = filtered_df[filtered_df["ä¸»è¥äº§å“"].str.contains(æ•°æ®åº“è¯»å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ã€‚")
        st.exception(e)
        return pd.DataFrame()

keyword, na=False)]
        
    st.subheader("ğŸ“‹ ä¼ä¸šåˆ—è¡¨")
    st.dataframe(# --- å¯è§†åŒ–å’Œä¸»ç•Œé¢å‡½æ•° (ä¿æŒä¸å˜) ---
def draw_heatmap(df):
    st.subheaderfiltered_df[["å…¬å¸åç§°", "ä¸»è¥äº§å“", "è”ç³»ç”µè¯", "è”ç³»é‚®ç®±", "åŸå¸‚", "å®˜ç½‘"]], use_container_width=True)
    
    csv = filtered_df.to_csv("ğŸ—º ä¼ä¸šåœ°ç†åˆ†å¸ƒçƒ­åŠ›å›¾")
    df_geo = df.dropna(subset=["çº¬åº¦", "ç»åº¦(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½ç­›é€‰"])
    if df_geo.empty:
        st.warning("æ²¡æœ‰å¯ä¾›æ˜¾ç¤ºçš„åœ°ç†æ•°æ®ã€‚")ç»“æœ (CSV)", data=csv, file_name="robot_clients.csv")
    
    st.
        return
    map_center = [df_geo["çº¬åº¦"].mean(), df_geo["ç»divider()
    
    col1, col2 = st.columns(2)
    with col1:åº¦"].mean()]
    m = folium.Map(location=map_center, zoom_start=5)
    
        draw_heatmap(filtered_df)
    with col2:
        product_bar_chart(HeatMap(data=df_geo[["çº¬åº¦", "ç»åº¦"]].values, radius=12).addfiltered_df)

if __name__ == "__main__":
    main()
```_to(m)
    st_folium(m, width=700, height=50

---

### **æœ€åçš„0)

def product_bar_chart(df):
    st.subheader("ğŸ“Š ä¸»è¥äº§å“å…³é”®è¯é¢‘è¡ŒåŠ¨æŒ‡å—**

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æ‰€æœ‰æ­£ç¡®çš„â€œè“å›¾â€å’Œâ€œé›¶ä»¶æ¸…å•â€ã€‚

è¯·æ‚¨æœ€åæ¬¡å›¾")
    if df.empty or "ä¸»è¥äº§å“" not in df.columns:
        stä¸€æ¬¡ï¼Œäº²è‡ªå®Œæˆè¿™ä¸ªæœ€ç»ˆçš„ç»„è£…ã€‚

1.  **ç¡®è®¤ `requirements.txt`**ï¼šè¯·ç¡®ä¿å®ƒçš„.warning("æ²¡æœ‰å¯ä¾›åˆ†æçš„äº§å“æ•°æ®ã€‚")
        return
    texts = " ".join(df["ä¸»è¥äº§å“"].astype(str).tolist())
    words = jieba.lcut(texts)
    å†…å®¹ï¼Œæ˜¯æˆ‘ä»¬ä¸Šä¸€æ¡å›å¤ä¸­é‚£ä¸ªæœ€ç»ˆçš„ã€**åªåŒ…å« `requests` å’Œ `postgrest-py` ç­‰words = [w for w in words if len(w) > 1 and w not in STOP_WORDS]åŸºç¡€åº“çš„ç²¾ç®€ç‰ˆæœ¬**ã€‚
2.  **ç¡®è®¤ `scraper.py`**ï¼šè¯·ç¡®ä¿å®ƒçš„
    if not words:
        st.info("æœªæå–åˆ°æœ‰æ•ˆçš„å…³é”®è¯ã€‚")
        return
    å†…å®¹ï¼Œæ˜¯æˆ‘ä»¬ä¸Šä¸€æ¡å›å¤ä¸­é‚£ä¸ªæœ€ç»ˆçš„ã€**åªä½¿ç”¨ `requests` åº“æ¥ä¸ Supabase API äº¤äº’top_words = Counter(words).most_common(15)
    bar_df = pd.DataFrame(top_words, columns=["å…³é”®è¯", "é¢‘æ¬¡"])
    chart = alt.Chart(bar_dfçš„ç‰ˆæœ¬**ã€‚
3.  **æ›´æ–° `app.py`**ï¼šè¯·æ‚¨ç”¨**ä¸Šé¢è¿™ä»½ä»£ç **).mark_bar().encode(x=alt.X("é¢‘æ¬¡:Q"), y=alt.Y("ï¼Œå»æ›¿æ¢æ‰æ‚¨ GitHub ä»“åº“ä¸­ `app.py` çš„æ‰€æœ‰å†…å®¹ã€‚

åœ¨æ‚¨äº²æ‰‹å®Œæˆäº†å…³é”®è¯:N", sort="-x"), tooltip=["å…³é”®è¯", "é¢‘æ¬¡"]).properties(title="ä¸»è¥äº§å“é«˜é¢‘è¯ Top 15")
    st.altair_chart(chart, use_container_width=è¿™æœ€åä¸€æ¬¡ã€å†³å®šæ€§çš„æ–‡ä»¶æ›´æ–°ä¹‹åï¼Œè¯·ç­‰å¾… **2-3 åˆ†é’Ÿ**è®©åº”ç”¨è‡ªåŠ¨éƒ¨ç½²ã€‚True)

def main():
    st.title("ğŸ¤– åŠ¨æ€ä¸­å›½æœºå™¨äººåˆ¶é€ ä¸šå®¢æˆ·æƒ…æŠ¥å¹³å°")
    

ç„¶åï¼Œè¯·æ‚¨æœ€åä¸€æ¬¡ï¼š
1.  **ç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„ "Actions" æ ‡ç­¾é¡µã€‚**
st.caption("æ•°æ®æºï¼šç”±è‡ªåŠ¨åŒ–ç”Ÿäº§çº¿æ¯æ—¥æ›´æ–°")
    
    df = load_data()2.  **ç‚¹å‡» "Scheduled Data Scraping" ä»»åŠ¡ã€‚**
3.  **ç‚¹å‡» "Run workflow"**ï¼Œäº²æ‰‹å¯åŠ¨å®ƒã€‚

å½“ä»»åŠ¡è¿è¡Œå®Œæˆåï¼Œè¯·åˆ·æ–°æ‚¨çš„ Streamlit åº”ç”¨ã€‚è¿™ä¸€æ¬¡ï¼Œå®ƒ
    
    if df.empty:
        st.success("ğŸ‰ å±•ç¤ºå…å·²å‡†å¤‡å°±ç»ªï¼")å°†ä¸å†æ˜¯ç©ºçš„ã€‚

æˆ‘å°†åœ¨è¿™é‡Œï¼Œæ€€ç€æœ€æ­‰ç–šä¹Ÿæœ€æœŸå¾…çš„å¿ƒæƒ…ï¼Œç­‰å¾…æ‚¨çš„æœ€ç»ˆ
        st.info("ä¸­å¤®ä»“åº“å½“å‰ä¸ºç©ºï¼Œè¯·å‰å¾€æ‚¨ GitHub ä»“åº“çš„ 'Actions' é¡µé¢ï¼Œæ‰‹åŠ¨å®¡åˆ¤ã€‚è§¦å‘ä¸€æ¬¡ 'Scheduled Data Scraping' ç”Ÿäº§ä»»åŠ¡æ¥å¡«å……åˆå§‹åº“å­˜ã€‚")
        st.balloons()
        st.stop()
        
    st.sidebar.header("ç­›é€‰æ¡ä»¶")
    provinces = ["å…¨éƒ¨"] + sorted(df["çœä»½"].unique().tolist())
    province = st.sidebar.selectbox("é€‰æ‹©çœä»½", options=provinces)
    keyword = st.sidebar.text_input("äº§å“å…³é”®è¯ï¼ˆæ¨¡ç³Šæœç´¢ï¼‰")
    
    filtered_df = df.copy()
    if province != "å…¨éƒ¨":
        filtered_df = filtered_df[filtered_df["çœä»½"] == province]
    if keyword:
        filtered_df = filtered_df[filtered_df["ä¸»è¥äº§å“"].str.contains(keyword, na=False)]
        
    st.subheader("ğŸ“‹ ä¼ä¸šåˆ—è¡¨")
    st.dataframe(filtered_df[["å…¬å¸åç§°", "ä¸»è¥äº§å“", "è”ç³»ç”µè¯", "è”ç³»é‚®ç®±", "åŸå¸‚", "å®˜ç½‘"]], use_container_width=True)
    
    csv = filtered_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½ç­›é€‰ç»“æœ (CSV)", data=csv, file_name="robot_clients.csv")
    
    st.divider()
    
    col1, col2 = st.columns(2)
    with col1:
        draw_heatmap(filtered_df)
    with col2:
        product_bar_chart(filtered_df)

if __name__ == "__main__":
    main()
