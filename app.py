import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import os
import time
import requests
from io import StringIO
import folium
from folium.plugins import HeatMap
from streamlit_folium import st_folium
import altair as alt
from collections import Counter
import jieba

# --- å¸¸é‡å®šä¹‰ ---
PERSISTENT_STORAGE_PATH = "/data"
DATA_FILE = os.path.join(PERSISTENT_STORAGE_PATH, "companies.parquet")
LAST_UPDATE_FILE = os.path.join(PERSISTENT_STORAGE_PATH, "last_update.txt")
UPDATE_INTERVAL_DAYS = 3

# --- æ ¸å¿ƒæ•°æ®é€»è¾‘ ---
def fetch_data_from_web():
    data_url = "https://gist.githubusercontent.com/ai-side-projects/74884107a729e240c4974751ed23811e/raw/companies_data.csv"
    print("å¼€å§‹ä» URL æŠ“å–æ•°æ®...")
    try:
        response = requests.get(data_url, timeout=30)
        response.raise_for_status()
        csv_data = StringIO(response.text)
        df = pd.read_csv(csv_data)
        print(f"æŠ“å–åˆ° {len(df)} æ¡æ•°æ®ã€‚")
        return df
    except requests.exceptions.RequestException as e:
        print(f"æ•°æ®æŠ“å–å¤±è´¥: {e}")
        return None

def check_and_update_data():
    now = datetime.utcnow()
    if not os.path.exists(DATA_FILE) or not os.path.exists(LAST_UPDATE_FILE):
         print("é¦–æ¬¡è¿è¡Œæˆ–æ•°æ®æ–‡ä»¶ä¸¢å¤±ï¼Œéœ€è¦æŠ“å–æ•°æ®ã€‚")
         df = fetch_data_from_web()
         if df is not None:
            df.to_parquet(DATA_FILE, index=False)
            with open(LAST_UPDATE_FILE, "w") as f:
                f.write(now.isoformat())
            return "FIRST_RUN", "é¦–æ¬¡è¿è¡Œï¼Œæ•°æ®å·²æˆåŠŸä¸‹è½½ï¼", df
         else:
            return "FAILED", "é¦–æ¬¡æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè¯·ç¨ååˆ·æ–°ã€‚", pd.DataFrame()

    with open(LAST_UPDATE_FILE, "r") as f:
        last_update_time = datetime.fromisoformat(f.read())

    if now - last_update_time > timedelta(days=UPDATE_INTERVAL_DAYS):
        print(f"æ•°æ®å·²è¿‡æœŸï¼Œéœ€è¦æ›´æ–°ã€‚")
        df = fetch_data_from_web()
        if df is not None:
            df.to_parquet(DATA_FILE, index=False)
            with open(LAST_UPDATE_FILE, "w") as f:
                f.write(now.isoformat())
            return "UPDATED", "æ•°æ®å·²æˆåŠŸæ›´æ–°ï¼", df
        else:
            return "FAILED", "æ•°æ®æ›´æ–°å¤±è´¥ï¼Œä½¿ç”¨æ—§æ•°æ®ã€‚", pd.read_parquet(DATA_FILE)

    print("æ•°æ®ä¸ºæœ€æ–°ã€‚")
    df = pd.read_parquet(DATA_FILE)
    return "UP_TO_DATE", "æ•°æ®ä¸ºæœ€æ–°ç‰ˆæœ¬ã€‚", df

# --- UI ç•Œé¢å‡½æ•° ---
STOP_WORDS = {"å…¬å¸", "æœ‰é™", "è´£ä»»", "æŠ€æœ¯", "ç§‘æŠ€", "å‘å±•", "çš„", "å’Œ", "ç­‰", "ä¸", "åŠ"}
def draw_heatmap(df):
    st.subheader("ğŸ—º ä¼ä¸šåœ°ç†åˆ†å¸ƒçƒ­åŠ›å›¾")
    df_geo = df.dropna(subset=["çº¬åº¦", "ç»åº¦"])
    if df_geo.empty:
        st.warning("æ— åœ°ç†æ•°æ®å¯ä¾›æ˜¾ç¤ºã€‚")
        return
    map_center = [df_geo["çº¬åº¦"].mean(), df_geo["ç»åº¦"].mean()]
    m = folium.Map(location=map_center, zoom_start=5)
    HeatMap(data=df_geo[["çº¬åº¦", "ç»åº¦"]].values, radius=12).add_to(m)
    st_folium(m, width=700, height=500)

def product_bar_chart(df):
    st.subheader("ğŸ“Š ä¸»è¥äº§å“å…³é”®è¯é¢‘æ¬¡å›¾")
    if df.empty:
        st.warning("æ— äº§å“æ•°æ®å¯ä¾›åˆ†æã€‚")
        return
    texts = " ".join(df["ä¸»è¥äº§å“"].astype(str).tolist())
    words = jieba.lcut(texts)
    words = [w for w in words if len(w) > 1 and w not in STOP_WORDS]
    top_words = Counter(words).most_common(15)
    if not top_words:
        st.info("æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯ã€‚")
        return
    bar_df = pd.DataFrame(top_words, columns=["å…³é”®è¯", "é¢‘æ¬¡"])
    chart = alt.Chart(bar_df).mark_bar().encode(
        x=alt.X("é¢‘æ¬¡:Q"),
        y=alt.Y("å…³é”®è¯:N", sort="-x"),
        tooltip=["å…³é”®è¯", "é¢‘æ¬¡"]
    ).properties(title="ä¸»è¥äº§å“é«˜é¢‘è¯ Top 15")
    st.altair_chart(chart, use_container_width=True)

# --- ä¸»ç¨‹åº ---
def main():
    st.set_page_config(page_title="åŠ¨æ€æœºå™¨äººå®¢æˆ·æƒ…æŠ¥å¹³å°", layout="wide")
    st.title("ğŸ¤– åŠ¨æ€ä¸­å›½æœºå™¨äººåˆ¶é€ ä¸šå®¢æˆ·æƒ…æŠ¥å¹³å°")
    st.caption("æ•°æ®ä¼šå®šæœŸè‡ªåŠ¨æ›´æ–°")

    with st.spinner("æ­£åœ¨åˆå§‹åŒ–å¹¶æ£€æŸ¥æ•°æ®ç‰ˆæœ¬..."):
        status, message, df = check_and_update_data()

    if status in ["FIRST_RUN", "UPDATED"]:
        st.success(message)
    elif status == "FAILED":
        st.error(message)

    if df.empty:
        st.warning("æ•°æ®å½“å‰ä¸ºç©ºï¼Œè¯·ç¨ååˆ·æ–°é‡è¯•ã€‚")
        return

    # UI ç•Œé¢æ¸²æŸ“
    st.sidebar.header("ç­›é€‰æ¡ä»¶")
    provinces = ["å…¨éƒ¨"] + sorted(df["çœä»½"].unique().tolist())
    province = st.sidebar.selectbox("é€‰æ‹©çœä»½", options=provinces)
    keyword = st.sidebar.text_input("äº§å“å…³é”®è¯ï¼ˆæ¨¡ç³Šæœç´¢ï¼‰")

    filtered_df = df.copy()
    if province != "å…¨éƒ¨":
        filtered_df = filtered_df[filtered_df["çœä»½"] == province]
    if keyword:
        filtered_df = filtered_df[filtered_df["ä¸»è¥äº§å“"].str.contains(keyword, na=False)]

    st.subheader("ğŸ“‹ ä¼ä¸šåˆ—è¡¨")
    st.dataframe(filtered_df[["å…¬å¸åç§°", "ä¸»è¥äº§å“", "è”ç³»ç”µè¯", "è”ç³»é‚®ç®±", "åŸå¸‚", "å®˜ç½‘"]], use_container_width=True)

    csv = filtered_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button("ğŸ“¥ ä¸‹è½½ç­›é€‰ç»“æœ (CSV)", data=csv, file_name="robot_clients.csv")

    st.divider()

    col1, col2 = st.columns(2)
    with col1:
        draw_heatmap(filtered_df)
    with col2:
        product_bar_chart(filtered_df)

if __name__ == "__main__":
    main()
